{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml, re, copy\n",
    "\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "import distributed\n",
    "from dask_ml.model_selection import GridSearchCV as GridSearchCVBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "client_gcs = storage.Client()\n",
    "bucket = client_gcs.get_bucket('djr-data')\n",
    "\n",
    "def gcs_to_df(f):\n",
    "    blob = bucket.blob(f)\n",
    "    buf = BytesIO()\n",
    "    blob.download_to_file(buf)\n",
    "    buf.seek(0)\n",
    "    return pd.read_csv(buf, encoding = \"utf-8\")\n",
    " \n",
    "df_train = gcs_to_df(\"kaggle-jigsaw/train.csv\")\n",
    "df_test = gcs_to_df(\"kaggle-jigsaw/test.csv\")\n",
    "yvar = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize client for interacting with dask\n",
    "# DASK_SCHEDULER_ADDRESS env variable specifies scheduler ip\n",
    "client_dask = distributed.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308619</td>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.266009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>0.308619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.286867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>0.115128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>0.266009</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>0.337736</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic   obscene    threat    insult  \\\n",
       "toxic          1.000000      0.308619  0.676515  0.157058  0.647518   \n",
       "severe_toxic   0.308619      1.000000  0.403014  0.123601  0.375807   \n",
       "obscene        0.676515      0.403014  1.000000  0.141179  0.741272   \n",
       "threat         0.157058      0.123601  0.141179  1.000000  0.150022   \n",
       "insult         0.647518      0.375807  0.741272  0.150022  1.000000   \n",
       "identity_hate  0.266009      0.201600  0.286867  0.115128  0.337736   \n",
       "\n",
       "               identity_hate  \n",
       "toxic               0.266009  \n",
       "severe_toxic        0.201600  \n",
       "obscene             0.286867  \n",
       "threat              0.115128  \n",
       "insult              0.337736  \n",
       "identity_hate       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation matrix\n",
    "df_train[yvar].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.095844\n",
       "severe_toxic     0.009996\n",
       "obscene          0.052948\n",
       "threat           0.002996\n",
       "insult           0.049364\n",
       "identity_hate    0.008805\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[yvar].apply(np.mean, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "xdata = df_train.comment_text\n",
    "ydata = df_train[yvar]\n",
    "xdata_train, xdata_eval, ydata_train, ydata_eval = train_test_split(xdata, ydata, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline\n",
    "pipeline = Pipeline(steps = [\n",
    "    ('cv', CountVectorizer(min_df=5, max_features = 50000, lowercase=False, strip_accents='unicode', stop_words='english', analyzer='word')), \n",
    "    ('tfidf', TfidfTransformer(sublinear_tf = True, use_idf = True)), \n",
    "    ('kbest', SelectKBest()),\n",
    "    ('model', LogisticRegression(class_weight = \"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non-multimetric, don't require refit = True for best_params_ / best_score_\n",
    "class GridSearchCV(GridSearchCVBase):\n",
    "\n",
    "    # For multiple metric evaluation, refit is a string denoting the scorer that should be \n",
    "    # used to find the best parameters for refitting the estimator \n",
    "    @property\n",
    "    def scorer_key(self):\n",
    "        return self.refit if self.multimetric_ else 'score'\n",
    "    \n",
    "    @property\n",
    "    def best_index(self):\n",
    "        check_is_fitted(self, 'cv_results_')\n",
    "        return np.flatnonzero(self.cv_results_['rank_test_{}'.format(self.scorer_key)] == 1)[0]\n",
    "\n",
    "    @property\n",
    "    def best_params_(self):\n",
    "        return self.cv_results_['params'][self.best_index]\n",
    "\n",
    "    @property\n",
    "    def best_score_(self):\n",
    "        return self.cv_results_['mean_test_{}'.format(self.scorer_key)][self.best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for toxic: {'cv__ngram_range': (1, 1), 'kbest__k': 'all', 'model__C': 0.1, 'model__penalty': 'l2', 'tfidf__norm': 'l2'}\n",
      "Best params score for toxic: 0.9581649926974369\n",
      "Best params for severe_toxic: {'cv__ngram_range': (1, 2), 'kbest__k': 10000, 'model__C': 0.1, 'model__penalty': 'l2', 'tfidf__norm': 'l2'}\n",
      "Best params score for severe_toxic: 0.984049857827509\n",
      "Best params for obscene: {'cv__ngram_range': (1, 1), 'kbest__k': 10000, 'model__C': 0.01, 'model__penalty': 'l1', 'tfidf__norm': None}\n",
      "Best params score for obscene: 0.9782391682556496\n",
      "Best params for threat: {'cv__ngram_range': (1, 1), 'kbest__k': 10000, 'model__C': 0.1, 'model__penalty': 'l2', 'tfidf__norm': 'l2'}\n",
      "Best params score for threat: 0.9794954551081121\n",
      "Best params for insult: {'cv__ngram_range': (1, 1), 'kbest__k': 25000, 'model__C': 0.1, 'model__penalty': 'l2', 'tfidf__norm': 'l2'}\n",
      "Best params score for insult: 0.9709854662517963\n",
      "Best params for identity_hate: {'cv__ngram_range': (1, 1), 'kbest__k': 10000, 'model__C': 0.1, 'model__penalty': 'l2', 'tfidf__norm': 'l2'}\n",
      "Best params score for identity_hate: 0.9673473415918085\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "param_grid = {\n",
    "    'cv__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__norm': ['l1', 'l2', None],\n",
    "    'kbest__k': [10000, 25000, \"all\"],\n",
    "    'model__C': [0.01, 0.1],\n",
    "    'model__penalty': ['l1', 'l2']\n",
    "}\n",
    "            \n",
    "try:\n",
    "    with open('model_param.yaml', 'r') as f:\n",
    "        param_optimal = yaml.load(f)\n",
    "except IOError:\n",
    "    param_optimal = {}\n",
    "\n",
    "    # create tuner\n",
    "    tuner = GridSearchCV(pipeline, param_grid, scheduler = client_dask, scoring = 'roc_auc', \n",
    "                         cv = 3, refit = False, return_train_score = False)\n",
    "    \n",
    "    # determine optimal hyperparameters\n",
    "    for y in yvar:\n",
    "        tuner.fit(xdata_train, ydata_train[y])\n",
    "        print('Best params for %s: %s' % (str(y), str(tuner.best_params_)))\n",
    "        print('Best params score for %s: %s' % (str(y), str(tuner.best_score_)))\n",
    "        param_optimal[y] = copy.deepcopy(tuner.best_params_)\n",
    "    \n",
    "    # save best params\n",
    "    with open('model_param.yaml', 'w') as f:\n",
    "        yaml.dump(param_optimal, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model with optimal param and generate predictions on eval set\n",
    "models = client_dask.map(lambda y: clone(pipeline).set_params(**param_optimal[y]).fit(xdata_train, ydata_train[y]), yvar)\n",
    "ydata_eval_pred = client_dask.map(lambda m: pd.Series(m.predict_proba(xdata_eval)[:,1]), models)\n",
    "\n",
    "ydata_eval_pred = client_dask.gather(ydata_eval_pred)\n",
    "ydata_eval_pred = pd.concat(ydata_eval_pred, axis = 1)\n",
    "ydata_eval_pred.columns = yvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AUCs: [0.961862557804696, 0.9851282088224007, 0.977699840822474, 0.9809085233663242, 0.970649479505864, 0.9732070152491284]\n",
      "Avg AUC: 0.9749092709284812\n"
     ]
    }
   ],
   "source": [
    "# calculate performance on eval set\n",
    "auc = [roc_auc_score(ydata_eval[y], ydata_eval_pred[y]) for y in yvar]\n",
    "print('Model AUCs: %s' % auc)\n",
    "print('Avg AUC: %s' % np.mean(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xdata_test = df_test.comment_text\n",
    "\n",
    "# models_final = client_dask.map(lambda y: clone(pipeline).fit(xdata, ydata[y]), yvar)\n",
    "# ydata_test_pred = client_dask.map(lambda m: pd.Series(m.predict_proba(xdata_test)[:,1]), models_final)\n",
    "\n",
    "# ydata_test_pred = client_dask.gather(ydata_test_pred)\n",
    "# ydata_test_pred = pd.concat(ydata_test_pred, axis = 1)\n",
    "# ydata_test_pred.columns = yvar\n",
    "# ydata_test_pred['id'] = df_test.id\n",
    "\n",
    "# ydata_test_pred.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
