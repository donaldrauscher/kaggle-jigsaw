{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, yaml\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from sklearn.feature_extraction.text import strip_tags, strip_accents_ascii, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, ParameterGrid\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from distributed import Client\n",
    "from dask_ml.model_selection import GridSearchCV as GridSearchCVBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df_train = pd.read_csv(\"data/train.csv\", encoding = \"utf-8\")\n",
    "df_test = pd.read_csv(\"data/train.csv\", encoding = \"utf-8\")\n",
    "yvar = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308619</td>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.266009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>0.308619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.286867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>0.115128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>0.266009</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>0.337736</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic   obscene    threat    insult  \\\n",
       "toxic          1.000000      0.308619  0.676515  0.157058  0.647518   \n",
       "severe_toxic   0.308619      1.000000  0.403014  0.123601  0.375807   \n",
       "obscene        0.676515      0.403014  1.000000  0.141179  0.741272   \n",
       "threat         0.157058      0.123601  0.141179  1.000000  0.150022   \n",
       "insult         0.647518      0.375807  0.741272  0.150022  1.000000   \n",
       "identity_hate  0.266009      0.201600  0.286867  0.115128  0.337736   \n",
       "\n",
       "               identity_hate  \n",
       "toxic               0.266009  \n",
       "severe_toxic        0.201600  \n",
       "obscene             0.286867  \n",
       "threat              0.115128  \n",
       "insult              0.337736  \n",
       "identity_hate       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation matrix\n",
    "df_train[yvar].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.095844\n",
       "severe_toxic     0.009996\n",
       "obscene          0.052948\n",
       "threat           0.002996\n",
       "insult           0.049364\n",
       "identity_hate    0.008805\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[yvar].apply(np.mean, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up W2V transformer\n",
    "class W2VTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, size = 100, **kwargs):\n",
    "        self.gensim_model = None\n",
    "        self.size = size\n",
    "        self.gensim_params = kwargs\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(doc):\n",
    "        doc = strip_tags(doc.lower())\n",
    "        doc = re.compile(r\"\\s\\s+\").sub(\" \", doc)\n",
    "        words = re.compile(r\"(?u)\\b\\w\\w+\\b\").findall(doc)\n",
    "        words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "        return words\n",
    "    \n",
    "    @property\n",
    "    def base_vector(self):\n",
    "        return np.zeros(self.gensim_model.vector_size)\n",
    "    \n",
    "    def get_vector_word(self, word):\n",
    "        try:\n",
    "            return self.gensim_model[word]\n",
    "        except KeyError:\n",
    "            return self.base_vector\n",
    "\n",
    "    def get_vector_sentence(self, sentence):\n",
    "        if sentence:\n",
    "            vectors = np.array([self.get_vector_word(w) for w in sentence])\n",
    "            return vectors.mean(axis = 0)\n",
    "        else:\n",
    "            return self.base_vector\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = X.apply(self.tokenize)\n",
    "        self.gensim_model = Word2Vec(sentences = sentences, size = self.size, **self.gensim_params)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.vstack([self.get_vector_sentence(s) for s in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "xdata = df_train.comment_text\n",
    "ydata = df_train[yvar]\n",
    "xdata_train, xdata_eval, ydata_train, ydata_eval = train_test_split(xdata, ydata, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline\n",
    "def basic_stats(docs):\n",
    "    nwords = docs.apply(lambda x: len(x.split()))\n",
    "    nchar = docs.apply(len)\n",
    "    ncap = docs.apply(lambda x: len(re.compile(r\"[A-Z]\").findall(x)))\n",
    "    ncap_perc = ncap / nchar\n",
    "    nexcl = docs.apply(lambda x: len(re.compile(r\"!\").findall(x)))\n",
    "    nquest = docs.apply(lambda x: len(re.compile(r\"\\?\").findall(x)))\n",
    "    nsymb = docs.apply(lambda x: len(re.compile(r\"&|@|#|\\$|%|\\*|\\^\").findall(x)))\n",
    "    nsmile = docs.apply(lambda x: len(re.compile(r\"((?::|;|=)(?:-)?(?:\\)|D|P))\").findall(x)))\n",
    "    return pd.DataFrame(data = dict(\n",
    "        nwords = nwords, nchar = nchar, ncap = ncap, ncap_perc = ncap_perc,\n",
    "        nexcl = nexcl, nquest = nquest, nsymb = nsymb, nsmile = nsmile\n",
    "    ))\n",
    "\n",
    "pipeline = Pipeline(steps = [\n",
    "    ('features', FeatureUnion(transformer_list = [\n",
    "        ('w2v', W2VTransformer()),\n",
    "        ('tfidf', TfidfVectorizer(min_df = 5, max_df = 0.5)),\n",
    "        ('kbest', Pipeline(steps = [\n",
    "            ('cv', CountVectorizer(min_df = 5, max_df = 0.5)),\n",
    "            ('kbest', SelectKBest())\n",
    "        ])),\n",
    "        ('stats', FunctionTransformer(func = basic_stats, validate = False))\n",
    "    ])),\n",
    "    ('model', XGBClassifier(seed = 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the parameter grid\n",
    "pg1 = [\n",
    "    {'features__w2v__size': [500], 'features__tfidf__max_features': [1000], 'features__kbest__kbest__k': [1000]},\n",
    "    {'features__w2v__size': [1000], 'features__tfidf__max_features': [2000], 'features__kbest__kbest__k': [2000]},\n",
    "    {'features__w2v__size': [1000], 'features__tfidf__max_features': [3000], 'features__kbest__kbest__k': [3000]},\n",
    "]\n",
    "pg2 = {\n",
    "    'model__n_estimators': [1000],\n",
    "    'model__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2],\n",
    "    'model__max_depth': [2, 4, 6, 8, 10],\n",
    "    'model__min_child_weight': [5],\n",
    "    'model__subsample': [1],\n",
    "    'model__colsample_bytree': [0.8]\n",
    "}\n",
    "\n",
    "listize = lambda x: dict(zip(x.keys(), [[i] for i in x.values()]))\n",
    "\n",
    "def merge_param_grids(x, y):\n",
    "    z = list(product(list(x), list(y)))\n",
    "    z = [dict(**i[0], **i[1]) for i in z]\n",
    "    z = [listize(i) for i in z]\n",
    "    return z\n",
    "\n",
    "pg = merge_param_grids(ParameterGrid(pg1), ParameterGrid(pg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non-multimetric, don't require refit = True for best_params_ / best_score_\n",
    "class GridSearchCV(GridSearchCVBase):\n",
    "\n",
    "    # For multiple metric evaluation, refit is a string denoting the scorer that should be \n",
    "    # used to find the best parameters for refitting the estimator \n",
    "    @property\n",
    "    def scorer_key(self):\n",
    "        return self.refit if self.multimetric_ else 'score'\n",
    "    \n",
    "    @property\n",
    "    def best_index(self):\n",
    "        check_is_fitted(self, 'cv_results_')\n",
    "        return np.flatnonzero(self.cv_results_['rank_test_{}'.format(self.scorer_key)] == 1)[0]\n",
    "\n",
    "    @property\n",
    "    def best_params_(self):\n",
    "        return self.cv_results_['params'][self.best_index]\n",
    "\n",
    "    @property\n",
    "    def best_score_(self):\n",
    "        return self.cv_results_['mean_test_{}'.format(self.scorer_key)][self.best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 s, sys: 5.97 s, total: 42.3 s\n",
      "Wall time: 7h 32min 33s\n",
      "Best params: {'features__kbest__kbest__k': 3000, 'features__tfidf__max_features': 3000, 'features__w2v__size': 1000, 'model__colsample_bytree': 0.8, 'model__learning_rate': 0.05, 'model__max_depth': 8, 'model__min_child_weight': 5, 'model__n_estimators': 1000, 'model__subsample': 1}\n",
      "Best params score: 0.9634738734674746\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "try:\n",
    "    with open('model_param.yaml', 'r') as f:\n",
    "        param_optimal = yaml.load(f)\n",
    "except IOError:\n",
    "    param_optimal = {}\n",
    "\n",
    "    # create tuner\n",
    "    client = Client()\n",
    "    tuner = GridSearchCV(pipeline, pg, scheduler = client, scoring = 'roc_auc', cv = 3, \n",
    "                         refit = False, return_train_score = False)\n",
    "    \n",
    "    # use tuner to determine optimal params\n",
    "    %time tuner.fit(xdata_train, ydata_train.toxic)\n",
    "    print('Best params: %s' % (str(tuner.best_params_)))\n",
    "    print('Best params score: %s' % (str(tuner.best_score_)))\n",
    "\n",
    "    # save best params\n",
    "    param_optimal = tuner.best_params_\n",
    "    with open('model_param.yaml', 'w') as f:\n",
    "        yaml.dump(param_optimal, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model with optimal param and generate predictions on eval set\n",
    "pipeline.set_params(**param_optimal)\n",
    "\n",
    "models = client.map(lambda y: clone(pipeline).fit(xdata_train, ydata_train[y]), yvar)\n",
    "ydata_eval_pred = client.map(lambda m: pd.Series(m.predict_proba(xdata_eval)[:,1]), models)\n",
    "\n",
    "ydata_eval_pred = client.gather(ydata_eval_pred)\n",
    "ydata_eval_pred = pd.concat(ydata_eval_pred, axis = 1)\n",
    "ydata_eval_pred.columns = yvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AUCs: [0.9691680716430817, 0.9803324363184136, 0.980444119652114, 0.9485788995068488, 0.9723553959950444, 0.9599720602518667]\n",
      "Avg AUC: 0.9684751638945617\n"
     ]
    }
   ],
   "source": [
    "# calculate performance on eval set\n",
    "auc = [roc_auc_score(ydata_eval[y], ydata_eval_pred[y]) for y in yvar]\n",
    "print('Model AUCs: %s' % auc)\n",
    "print('Avg AUC: %s' % np.mean(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata_test = df_test.comment_text\n",
    "\n",
    "models_final = client.map(lambda y: clone(pipeline).fit(xdata, ydata[y]), yvar)\n",
    "ydata_test_pred = client.map(lambda m: pd.Series(m.predict_proba(xdata_test)[:,1]), models_final)\n",
    "\n",
    "ydata_test_pred = client.gather(ydata_test_pred)\n",
    "ydata_test_pred = pd.concat(ydata_test_pred, axis = 1)\n",
    "ydata_test_pred.columns = yvar\n",
    "ydata_test_pred['id'] = df_test.id\n",
    "\n",
    "ydata_test_pred.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
