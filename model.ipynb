{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml, re\n",
    "\n",
    "from google.cloud import storage\n",
    "from io import BytesIO\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, strip_tags\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "import distributed\n",
    "from dask_ml.model_selection import GridSearchCV as GridSearchCVBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "client_gcs = storage.Client()\n",
    "bucket = client_gcs.get_bucket('djr-data')\n",
    "\n",
    "def gcs_to_df(f):\n",
    "    blob = bucket.blob(f)\n",
    "    buf = BytesIO()\n",
    "    blob.download_to_file(buf)\n",
    "    buf.seek(0)\n",
    "    return pd.read_csv(buf, encoding = \"utf-8\")\n",
    " \n",
    "df_train = gcs_to_df(\"kaggle-jigsaw/train.csv\")\n",
    "df_test = gcs_to_df(\"kaggle-jigsaw/test.csv\")\n",
    "yvar = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize client for interacting with dask\n",
    "# DASK_SCHEDULER_ADDRESS env variable specifies scheduler ip\n",
    "client_dask = distributed.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308619</td>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.266009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>0.308619</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>0.676515</td>\n",
       "      <td>0.403014</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.286867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>0.157058</td>\n",
       "      <td>0.123601</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>0.115128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>0.647518</td>\n",
       "      <td>0.375807</td>\n",
       "      <td>0.741272</td>\n",
       "      <td>0.150022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.337736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>0.266009</td>\n",
       "      <td>0.201600</td>\n",
       "      <td>0.286867</td>\n",
       "      <td>0.115128</td>\n",
       "      <td>0.337736</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  toxic  severe_toxic   obscene    threat    insult  \\\n",
       "toxic          1.000000      0.308619  0.676515  0.157058  0.647518   \n",
       "severe_toxic   0.308619      1.000000  0.403014  0.123601  0.375807   \n",
       "obscene        0.676515      0.403014  1.000000  0.141179  0.741272   \n",
       "threat         0.157058      0.123601  0.141179  1.000000  0.150022   \n",
       "insult         0.647518      0.375807  0.741272  0.150022  1.000000   \n",
       "identity_hate  0.266009      0.201600  0.286867  0.115128  0.337736   \n",
       "\n",
       "               identity_hate  \n",
       "toxic               0.266009  \n",
       "severe_toxic        0.201600  \n",
       "obscene             0.286867  \n",
       "threat              0.115128  \n",
       "insult              0.337736  \n",
       "identity_hate       1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation matrix\n",
    "df_train[yvar].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.095844\n",
       "severe_toxic     0.009996\n",
       "obscene          0.052948\n",
       "threat           0.002996\n",
       "insult           0.049364\n",
       "identity_hate    0.008805\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[yvar].apply(np.mean, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "xdata = df_train.comment_text\n",
    "ydata = df_train[yvar]\n",
    "xdata_train, xdata_eval, ydata_train, ydata_eval = train_test_split(xdata, ydata, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return words from corpus\n",
    "# TODO: also try r\"([\\w][\\w']*\\w)\"\n",
    "def tokenize(doc, token=r\"(?u)\\b\\w\\w+\\b\"):\n",
    "    doc = strip_tags(doc.lower())\n",
    "    doc = re.compile(r\"\\s\\s+\").sub(\" \", doc)\n",
    "    words = re.compile(token).findall(doc)\n",
    "    return words\n",
    "\n",
    "\n",
    "# remove stop words\n",
    "def remove_stop_words(x, stop_words=ENGLISH_STOP_WORDS):\n",
    "    return [i for i in x if i not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for gensim Phraser\n",
    "COMMON_TERMS = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\"]\n",
    "class PhraseTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, common_terms=COMMON_TERMS):\n",
    "        self.phraser = None\n",
    "        self.common_terms = common_terms\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        phrases = Phrases(X, common_terms=self.common_terms)\n",
    "        self.phraser = Phraser(phrases)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(lambda x: self.phraser[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for making tagged documents\n",
    "# NOTE: can't use FunctionTransformer since TransformerMixin doesn't pass y to fit_transform anymore\n",
    "class MakeTaggedDocuments(BaseEstimator):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if y is not None:\n",
    "            yvar = list(y.columns)\n",
    "            tags = y.apply(lambda row: [i for i,j in zip(yvar, row) if j == 1], axis=1)\n",
    "            return [TaggedDocument(words=w, tags=t) for w,t in zip(X, tags)]\n",
    "        else:\n",
    "            return [TaggedDocument(words=w, tags=[]) for w in X]\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        return self.transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for gensim Doc2Vec\n",
    "class D2VEstimator(BaseEstimator):\n",
    "\n",
    "    def __init__(self, min_count=10, alpha=0.025, min_alpha=0.0001, vector_size=200, dm=0, epochs=20):\n",
    "        self.min_count = min_count\n",
    "        self.alpha = alpha\n",
    "        self.min_alpha = min_alpha\n",
    "        self.vector_size = vector_size\n",
    "        self.dm = dm\n",
    "        self.epochs = epochs\n",
    "        self.yvar = None\n",
    "        self.model = Doc2Vec(seed=1, hs=1, negative=0, dbow_words=0,\n",
    "                             min_count=self.min_count, alpha=self.alpha, min_alpha=self.min_alpha,\n",
    "                             vector_size=self.vector_size, dm=self.dm, epochs=self.epochs)\n",
    "\n",
    "    def get_tags(self, doc):\n",
    "        vec = self.model.infer_vector(doc.words, self.model.alpha, self.model.min_alpha, self.model.epochs)\n",
    "        return dict(self.model.docvecs.most_similar([vec]))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model.build_vocab(X)\n",
    "        self.model.train(X, epochs=self.model.epochs, total_examples=self.model.corpus_count)\n",
    "        self.model.delete_temporary_training_data()\n",
    "        self.yvar = list(y.columns)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = [self.get_tags(d) for d in X]\n",
    "        pred = pd.DataFrame.from_records(data=pred)\n",
    "        return pred[self.yvar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blend predictions from multiple models\n",
    "class Blender(FeatureUnion):\n",
    "\n",
    "    def __init__(self, transformer_list, n_jobs=1, transformer_weights=None):\n",
    "        self.transformer_list = transformer_list\n",
    "        self.scaler_list = [(t, StandardScaler()) for t, _ in transformer_list]\n",
    "        self.n_jobs = n_jobs\n",
    "        default_transformer_weights = [1.0/len(transformer_list) for i,j in transformer_list]\n",
    "        self.transformer_weights = transformer_weights if transformer_weights else default_transformer_weights\n",
    "\n",
    "    @property\n",
    "    def transformer_weights(self):\n",
    "        return self._transformer_weights\n",
    "        \n",
    "    @transformer_weights.setter\n",
    "    def transformer_weights(self, values):\n",
    "        self._transformer_weights = {t[0]:v for t,v in zip(self.transformer_list, values)}\n",
    "\n",
    "    # don't need to check for fit and transform\n",
    "    def _validate_transformers(self):\n",
    "        pass\n",
    "\n",
    "    # iterator with scalers\n",
    "    def _iter_ss(self):\n",
    "        get_weight = (self.transformer_weights or {}).get\n",
    "        return [(t[0], t[1], s[1], get_weight(t[0])) for t, s in zip(self.transformer_list, self.scaler_list)]\n",
    "\n",
    "    # also fit scalers\n",
    "    def fit(self, X, y):\n",
    "        super(Blender, self).fit(X, y)\n",
    "        self.scaler_list = [(name, ss.fit(trans.predict_proba(X))) for name, trans, ss, _ in self._iter_ss()]\n",
    "        return self\n",
    "\n",
    "    # generate probabilities\n",
    "    def predict_proba(self, X):\n",
    "        Xs = [ss.transform(trans.predict_proba(X))*weight for name, trans, ss, weight in self._iter_ss()]\n",
    "        return np.sum(Xs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline\n",
    "d2v_pipeline = Pipeline(steps=[\n",
    "    ('tk', FunctionTransformer(func=lambda x: x.apply(tokenize), validate=False)),\n",
    "    ('ph', PhraseTransformer()),\n",
    "    ('sw', FunctionTransformer(func=lambda x: x.apply(remove_stop_words), validate=False)),\n",
    "    ('doc', MakeTaggedDocuments()),\n",
    "    ('d2v', D2VEstimator())\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('cv', CountVectorizer(min_df=5, max_features=50000, strip_accents='unicode',\n",
    "                           stop_words='english', analyzer='word')),\n",
    "    ('tfidf', TfidfTransformer(sublinear_tf=True, use_idf=True)),\n",
    "    ('lr', OneVsRestClassifier(LogisticRegression(class_weight=\"balanced\")))\n",
    "])\n",
    "\n",
    "pipeline = Blender(transformer_list=[('d2v', d2v_pipeline), ('lr', lr_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non-multimetric, don't require refit = True for best_params_ / best_score_\n",
    "class GridSearchCV(GridSearchCVBase):\n",
    "\n",
    "    # For multiple metric evaluation, refit is a string denoting the scorer that should be \n",
    "    # used to find the best parameters for refitting the estimator \n",
    "    @property\n",
    "    def scorer_key(self):\n",
    "        return self.refit if self.multimetric_ else 'score'\n",
    "    \n",
    "    @property\n",
    "    def best_index(self):\n",
    "        check_is_fitted(self, 'cv_results_')\n",
    "        return np.flatnonzero(self.cv_results_['rank_test_{}'.format(self.scorer_key)] == 1)[0]\n",
    "\n",
    "    @property\n",
    "    def best_params_(self):\n",
    "        return self.cv_results_['params'][self.best_index]\n",
    "\n",
    "    @property\n",
    "    def best_score_(self):\n",
    "        return self.cv_results_['mean_test_{}'.format(self.scorer_key)][self.best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some functions for dealing with parameter grids\n",
    "def add_prefix(prefix, x):\n",
    "    return {'{}__{}'.format(prefix, k):v for k,v in x.items()}\n",
    "\n",
    "def flatten_dict(x):\n",
    "    temp = {}\n",
    "    for k,v in x.items():\n",
    "        if isinstance(v, dict):\n",
    "            temp.update(add_prefix(k, flatten_dict(v.copy())))\n",
    "        else:\n",
    "            temp.update({k: v})\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'d2v__alpha': 0.025, 'd2v__epochs': 30, 'd2v__min_count': 10, 'd2v__vector_size': 200}\n",
      "Best params score: 0.9520673206887134\n",
      "Best params: {'cv__lowercase': True, 'cv__ngram_range': (1, 1), 'lr__estimator__C': 0.1, 'lr__estimator__penalty': 'l2', 'tfidf__norm': 'l2'}\n",
      "Best params score: 0.9764642394949188\n",
      "Best params: {'transformer_weights': (0.3, 0.7)}\n",
      "Best params score: 0.9774035665175447\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "param_grid = {\n",
    "    'd2v': {\n",
    "        'd2v__min_count': [10, 25],\n",
    "        'd2v__alpha': [0.025, 0.05],\n",
    "        'd2v__epochs': [10, 20, 30],\n",
    "        'd2v__vector_size': [200, 300]        \n",
    "    }, \n",
    "    'lr': {\n",
    "        'cv__lowercase': [True, False],\n",
    "        'cv__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__norm': ['l1', 'l2', None],\n",
    "        'lr__estimator__C': [0.01, 0.1],\n",
    "        'lr__estimator__penalty': ['l1', 'l2']        \n",
    "    },\n",
    "    'blender': {\n",
    "        'transformer_weights': [(0.3, 0.7), (0.4, 0.6), (0.5, 0.5), (0.6, 0.4), (0.7, 0.3)]        \n",
    "    }\n",
    "}\n",
    "\n",
    "# wrapper for hyperparameter tuning\n",
    "def hyperparameter_tune(pipeline, param_grid):\n",
    "    # create tuner\n",
    "    tuner = GridSearchCV(pipeline, param_grid, scheduler=client_dask, scoring='roc_auc', \n",
    "                         cv=3, refit=False, return_train_score=False)\n",
    "    \n",
    "    # determine optimal hyperparameters\n",
    "    tuner.fit(xdata_train, ydata_train)\n",
    "    print('Best params: %s' % (str(tuner.best_params_)))\n",
    "    print('Best params score: %s' % (str(tuner.best_score_)))\n",
    "    \n",
    "    return tuner.best_params_\n",
    "\n",
    "# load saved hyperparameters if available; o.w. tune\n",
    "try:\n",
    "    with open('model_param_d2v.yaml', 'r') as f:\n",
    "        param_optimal = yaml.load(f)\n",
    "    \n",
    "except IOError:\n",
    "    param_optimal = {}\n",
    "    \n",
    "    # tune each model\n",
    "    param_optimal['d2v'] = hyperparameter_tune(d2v_pipeline, param_grid['d2v'])\n",
    "    param_optimal['lr'] = hyperparameter_tune(lr_pipeline, param_grid['lr'])\n",
    "    \n",
    "    # tune blender\n",
    "    d2v_pipeline.set_params(**param_optimal['d2v'])\n",
    "    lr_pipeline.set_params(**param_optimal['lr'])\n",
    "    param_optimal.update(hyperparameter_tune(pipeline, param_grid['blender']))\n",
    "    \n",
    "    # flatten\n",
    "    param_optimal = flatten_dict(param_optimal)\n",
    "    \n",
    "    # save best params\n",
    "    with open('model_param_d2v.yaml', 'w') as f:\n",
    "        yaml.dump(param_optimal, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blender(n_jobs=1,\n",
       "    transformer_list=[('d2v', Pipeline(memory=None,\n",
       "     steps=[('tk', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function <lambda> at 0x7f39416d12f0>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "          validate=False)), ('ph', PhraseTransformer(com...ne,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=1))]))],\n",
       "    transformer_weights={'d2v': 0.3, 'lr': 0.7})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build model with optimal param\n",
    "pipeline.set_params(**param_optimal)\n",
    "pipeline.fit(xdata_train, ydata_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to eval set\n",
    "ydata_eval_pred = pipeline.predict_proba(xdata_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model AUCs: [0.9662283198414882, 0.9857095145804597, 0.982421955124849, 0.9849362663053255, 0.9757783792333873, 0.9768901227451926]\n",
      "Avg AUC: 0.9786607596384505\n"
     ]
    }
   ],
   "source": [
    "# calculate auc\n",
    "auc = [roc_auc_score(ydata_eval[y], ydata_eval_pred[:,i]) for i,y in enumerate(yvar)]\n",
    "print('Model AUCs: %s' % auc)\n",
    "print('Avg AUC: %s' % np.mean(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Blender(n_jobs=1,\n",
       "    transformer_list=[('d2v', Pipeline(memory=None,\n",
       "     steps=[('tk', FunctionTransformer(accept_sparse=False,\n",
       "          func=<function <lambda> at 0x7f39416d12f0>, inv_kw_args=None,\n",
       "          inverse_func=None, kw_args=None, pass_y='deprecated',\n",
       "          validate=False)), ('ph', PhraseTransformer(com...ne,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=1))]))],\n",
       "    transformer_weights={'d2v': 0.3, 'lr': 0.7})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate final model\n",
    "pipeline_final = clone(pipeline)\n",
    "pipeline_final.set_params(**param_optimal)\n",
    "pipeline_final.fit(xdata, ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output\n",
    "xdata_test = df_test.comment_text\n",
    "ydata_test_pred = pipeline_final.predict_proba(xdata_test)\n",
    "ydata_test_pred = pd.DataFrame(data=ydata_test_pred, columns=yvar)\n",
    "ydata_test_pred['id'] = df_test.id\n",
    "ydata_test_pred.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
