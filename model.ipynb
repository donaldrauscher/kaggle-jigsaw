{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re, yaml\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from distributed import Client\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.feature_extraction.text import strip_tags, strip_accents_ascii, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from dask_ml.model_selection import RandomizedSearchCV as RandomizedSearchCVBase\n",
    "\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv(\"data/train.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up W2V transformer\n",
    "class W2VTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, size = 100, **kwargs):\n",
    "        self.gensim_model = None\n",
    "        self.size = size\n",
    "        self.gensim_params = kwargs\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(doc):\n",
    "        doc = strip_tags(doc.lower())\n",
    "        doc = re.compile(r\"\\s\\s+\").sub(\" \", doc)\n",
    "        words = re.compile(r\"(?u)\\b\\w\\w+\\b\").findall(doc)\n",
    "        words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "        return words\n",
    "    \n",
    "    @property\n",
    "    def base_vector(self):\n",
    "        return np.zeros(self.gensim_model.vector_size)\n",
    "    \n",
    "    def get_vector_word(self, word):\n",
    "        try:\n",
    "            return self.gensim_model[word]\n",
    "        except KeyError:\n",
    "            return self.base_vector\n",
    "\n",
    "    def get_vector_sentence(self, sentence):\n",
    "        if sentence:\n",
    "            vectors = np.array([self.get_vector_word(w) for w in sentence])\n",
    "            return vectors.mean(axis = 0)\n",
    "        else:\n",
    "            return self.base_vector\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = X.apply(self.tokenize)\n",
    "        self.gensim_model = Word2Vec(sentences = sentences, size = self.size, **self.gensim_params)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.vstack([self.get_vector_sentence(s) for s in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "xdata = df.comment_text\n",
    "ydata = df.toxic\n",
    "xdata_train, xdata_test, ydata_train, ydata_test = train_test_split(xdata, ydata, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline\n",
    "def basic_stats(docs):\n",
    "    nwords = docs.apply(lambda x: len(x.split()))\n",
    "    nchar = docs.apply(len)\n",
    "    ncap = docs.apply(lambda x: len(re.compile(r\"[A-Z]\").findall(x)))\n",
    "    ncap_perc = ncap / nchar\n",
    "    nexcl = docs.apply(lambda x: len(re.compile(r\"!\").findall(x)))\n",
    "    nquest = docs.apply(lambda x: len(re.compile(r\"\\?\").findall(x)))\n",
    "    nsymb = docs.apply(lambda x: len(re.compile(r\"&|@|#|\\$|%|\\*|\\^\").findall(x)))\n",
    "    nsmile = docs.apply(lambda x: len(re.compile(r\"((?::|;|=)(?:-)?(?:\\)|D|P))\").findall(x)))\n",
    "    return pd.DataFrame(data = dict(\n",
    "        nwords = nwords, nchar = nchar, ncap = ncap, ncap_perc = ncap_perc,\n",
    "        nexcl = nexcl, nquest = nquest, nsymb = nsymb, nsmile = nsmile\n",
    "    ))\n",
    "\n",
    "pipeline = Pipeline(steps = [\n",
    "    ('features', FeatureUnion(transformer_list = [\n",
    "        ('w2v', W2VTransformer(workers = 1)),\n",
    "        ('tfidf', TfidfVectorizer(min_df = 3, max_df = 0.5)),\n",
    "        ('kbest', Pipeline(steps = [\n",
    "            ('cv', CountVectorizer(min_df = 3, max_df = 0.5)),\n",
    "            ('kbest', SelectKBest())\n",
    "        ])),\n",
    "        ('stats', FunctionTransformer(func = basic_stats, validate = False)),\n",
    "    ])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('model', LogisticRegression(class_weight = \"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.14 s, sys: 754 ms, total: 5.9 s\n",
      "Wall time: 56min 14s\n",
      "Best params: {'features__kbest__kbest__k': 500, 'features__tfidf__max_features': 750, 'features__w2v__size': 750, 'model__C': 0.01, 'model__penalty': 'l1'}\n",
      "Best params score: 0.9541523596852568\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "param_grid = {\n",
    "    'features__w2v__size': (np.arange(1, 4) * 250).tolist(),\n",
    "    'features__tfidf__max_features': (np.arange(1, 4) * 250).tolist(),\n",
    "    'features__kbest__kbest__k': (np.arange(1, 8) * 250).tolist(),\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__C': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# for non-multimetric, don't require refit = True for best_params_ / best_score_\n",
    "class RandomizedSearchCV(RandomizedSearchCVBase):\n",
    "\n",
    "    # For multiple metric evaluation, refit is a string denoting the scorer that should be \n",
    "    # used to find the best parameters for refitting the estimator \n",
    "    @property\n",
    "    def scorer_key(self):\n",
    "        return self.refit if self.multimetric_ else 'score'\n",
    "    \n",
    "    @property\n",
    "    def best_index(self):\n",
    "        check_is_fitted(self, 'cv_results_')\n",
    "        return np.flatnonzero(self.cv_results_['rank_test_{}'.format(self.scorer_key)] == 1)[0]\n",
    "\n",
    "    @property\n",
    "    def best_params_(self):\n",
    "        return self.cv_results_['params'][self.best_index]\n",
    "\n",
    "    @property\n",
    "    def best_score_(self):\n",
    "        return self.cv_results_['mean_test_{}'.format(self.scorer_key)][self.best_index]\n",
    "\n",
    "\n",
    "try:\n",
    "    with open('model_param.yaml', 'r') as f:\n",
    "        param_optimal = yaml.load(f)\n",
    "except IOError:\n",
    "    param_optimal = {}\n",
    "\n",
    "    # create tuner\n",
    "    client = Client()\n",
    "    ss = ShuffleSplit(n_splits = 5, train_size = 0.25, test_size = 0.25, random_state = 1)\n",
    "    tuner = RandomizedSearchCV(pipeline, param_grid, scheduler = client, scoring = 'roc_auc', \n",
    "                               cv = ss, refit = False, return_train_score = False, random_state = 1, \n",
    "                               n_iter = 20)\n",
    "    \n",
    "    # use tuner to determine optimal params\n",
    "    %time tuner.fit(xdata_train, ydata_train)\n",
    "    print('Best params: %s' % (str(tuner.best_params_)))\n",
    "    print('Best params score: %s' % (str(tuner.best_score_)))\n",
    "\n",
    "    # save best params\n",
    "    param_optimal = tuner.best_params_\n",
    "    with open('model_param.yaml', 'w') as f:\n",
    "        yaml.dump(param_optimal, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 24s, sys: 3.75 s, total: 15min 27s\n",
      "Wall time: 15min 25s\n"
     ]
    }
   ],
   "source": [
    "# build model with optimal param\n",
    "pipeline.set_params(**param_optimal)\n",
    "%time model = pipeline.fit(xdata_train, ydata_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 445 ms, total: 2min 56s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "# make predictions for our test set\n",
    "%time ydata_test_pred = model.predict_proba(xdata_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive threshold: 0.7494811580366171\n",
      "Confusion matrix:\n",
      "[[27997   815]\n",
      " [  815  2288]]\n",
      "AUC: 0.9582230413984506\n"
     ]
    }
   ],
   "source": [
    "# determine cutoff balancing precision/recall\n",
    "precision, recall, threshold = precision_recall_curve(ydata_test, ydata_test_pred)\n",
    "pos_threshold = np.min(threshold[precision[1:] > recall[:-1]])\n",
    "print('Positive threshold: %s' % str(pos_threshold))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(ydata_test, (ydata_test_pred >= pos_threshold).astype(int)))\n",
    "print('AUC: %s' % roc_auc_score(ydata_test, ydata_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
