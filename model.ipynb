{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re, yaml\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "#import distributed.joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.feature_extraction.text import strip_tags, strip_accents_ascii, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, ShuffleSplit\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, roc_auc_score\n",
    "#from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv(\"data/train.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up W2V transformer\n",
    "class W2VTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, size = 100, **kwargs):\n",
    "        self.gensim_model = None\n",
    "        self.size = size\n",
    "        self.gensim_params = kwargs\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.gensim_model = Word2Vec(sentences = X, size = self.size, **self.gensim_params)\n",
    "        return self\n",
    "\n",
    "    @property\n",
    "    def base_vector(self):\n",
    "        return np.zeros(self.gensim_model.vector_size)\n",
    "    \n",
    "    def get_vector_word(self, word):\n",
    "        try:\n",
    "            return self.gensim_model[word]\n",
    "        except KeyError:\n",
    "            return self.base_vector\n",
    "\n",
    "    def get_vector_sentence(self, sentence):\n",
    "        if sentence:\n",
    "            vectors = np.array([self.get_vector_word(w) for w in sentence])\n",
    "            return vectors.mean(axis = 0)\n",
    "        else:\n",
    "            return self.base_vector\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.vstack([self.get_vector_sentence(s) for s in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "xdata = df.comment_text\n",
    "ydata = df.toxic\n",
    "xdata_train, xdata_test, ydata_train, ydata_test = train_test_split(xdata, ydata, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline\n",
    "def tokenize(doc):\n",
    "    doc = strip_tags(doc.lower())\n",
    "    doc = re.compile(r\"\\s\\s+\").sub(\" \", doc)\n",
    "    words = re.compile(r\"(?u)\\b\\w\\w+\\b\").findall(doc)\n",
    "    words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "    return words\n",
    "\n",
    "pipeline = Pipeline(steps = [\n",
    "    ('features', FeatureUnion(transformer_list = [\n",
    "        ('w2v', Pipeline(steps = [\n",
    "            ('token', FunctionTransformer(func = lambda X: X.apply(tokenize), validate = False)),\n",
    "            ('w2v', W2VTransformer())            \n",
    "        ])),\n",
    "        ('tfidf', TfidfVectorizer(min_df = 3, max_df = 0.5)),\n",
    "        ('kbest', Pipeline(steps = [\n",
    "            ('cv', CountVectorizer(min_df = 3, max_df = 0.5)),\n",
    "            ('kbest', SelectKBest())\n",
    "        ])),\n",
    "    ])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('model', LogisticRegression(class_weight = \"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donald/Documents/kaggle-jigsaw/venv/local/lib/python2.7/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "param_grid = {\n",
    "    'features__w2v__w2v__size': (np.arange(1, 10) * 100).tolist(),\n",
    "    'features__tfidf__max_features': (np.arange(1, 10) * 100).tolist(),\n",
    "    'features__kbest__kbest__k': (np.arange(1, 8) * 250).tolist(),\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__C': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open('model_param.yaml', 'r') as f:\n",
    "        param_optimal = yaml.load(f)\n",
    "except IOError:\n",
    "    param_optimal = {}\n",
    "\n",
    "    # create tuner\n",
    "    ss = ShuffleSplit(n_splits = 5, train_size = 0.25, random_state = 1)\n",
    "    tuner = RandomizedSearchCV(pipeline, param_grid, scoring = 'roc_auc', cv = ss, verbose = 1, refit = False, \n",
    "                               random_state = 1, n_iter = 20)\n",
    "    \n",
    "#     # use tuner to determine optimal params\n",
    "#     # NOTE: need to replace localhost with cluster IP\n",
    "#     with parallel_backend('dask.distributed', scheduler_host='localhost:8786', \n",
    "#                           scatter=[xdata_train, ydata_train]):\n",
    "#         tuner.fit(xdata_train, ydata_train)\n",
    "#     print('Best params: %s' % (str(tuner.best_params_)))\n",
    "#     print('Best params score: %s' % (str(tuner.best_score_)))\n",
    "\n",
    "#     # save best params\n",
    "#     param_optimal.update(tuner.best_params_, model_name)\n",
    "\n",
    "#     with open('model_param.yaml', 'w') as f:\n",
    "#         yaml.dump(param_optimal, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donald/Documents/kaggle-jigsaw/venv/lib/python2.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# build model with optimal param\n",
    "param_optimal = {\n",
    "    'features__w2v__w2v__size': 500,\n",
    "    'features__tfidf__max_features': 500,\n",
    "    'features__kbest__kbest__k': 2000,\n",
    "    'model__penalty': 'l2',\n",
    "    'model__C': 0.01\n",
    "}\n",
    "pipeline.set_params(**param_optimal)\n",
    "model = pipeline.fit(xdata_train, ydata_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donald/Documents/kaggle-jigsaw/venv/lib/python2.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# make predictions for our test set\n",
    "ydata_test_pred = model.predict_proba(xdata_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive threshold: 0.818817638904037\n",
      "Confusion matrix:\n",
      "[[28072   740]\n",
      " [  741  2362]]\n",
      "AUC: 0.9627166282140919\n"
     ]
    }
   ],
   "source": [
    "# determine cutoff balancing precision/recall\n",
    "precision, recall, threshold = precision_recall_curve(ydata_test, ydata_test_pred)\n",
    "pos_threshold = np.min(threshold[precision[1:] > recall[:-1]])\n",
    "print('Positive threshold: %s' % str(pos_threshold))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(ydata_test, (ydata_test_pred >= pos_threshold).astype(int)))\n",
    "print('AUC: %s' % roc_auc_score(ydata_test, ydata_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv (Kaggle Jigsaw)",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
