{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re, yaml\n",
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "from distributed import Client\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.feature_extraction.text import strip_tags, strip_accents_ascii, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import precision_recall_curve, confusion_matrix, roc_auc_score\n",
    "\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "                                                \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv(\"data/train.csv\", encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up W2V transformer\n",
    "class W2VTransformer(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, size = 100, **kwargs):\n",
    "        self.gensim_model = None\n",
    "        self.size = size\n",
    "        self.gensim_params = kwargs\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(doc):\n",
    "        doc = strip_tags(doc.lower())\n",
    "        doc = re.compile(r\"\\s\\s+\").sub(\" \", doc)\n",
    "        words = re.compile(r\"(?u)\\b\\w\\w+\\b\").findall(doc)\n",
    "        words = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "        return words\n",
    "    \n",
    "    @property\n",
    "    def base_vector(self):\n",
    "        return np.zeros(self.gensim_model.vector_size)\n",
    "    \n",
    "    def get_vector_word(self, word):\n",
    "        try:\n",
    "            return self.gensim_model[word]\n",
    "        except KeyError:\n",
    "            return self.base_vector\n",
    "\n",
    "    def get_vector_sentence(self, sentence):\n",
    "        if sentence:\n",
    "            vectors = np.array([self.get_vector_word(w) for w in sentence])\n",
    "            return vectors.mean(axis = 0)\n",
    "        else:\n",
    "            return self.base_vector\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        sentences = X.apply(self.tokenize)\n",
    "        self.gensim_model = Word2Vec(sentences = sentences, size = self.size, **self.gensim_params)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return np.vstack([self.get_vector_sentence(s) for s in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test split\n",
    "xdata = df.comment_text\n",
    "ydata = df.toxic\n",
    "xdata_train, xdata_test, ydata_train, ydata_test = train_test_split(xdata, ydata, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline\n",
    "def basic_stats(docs):\n",
    "    nwords = docs.apply(lambda x: len(x.split()))\n",
    "    nchar = docs.apply(len)\n",
    "    ncap = docs.apply(lambda x: len(re.compile(r\"[A-Z]\").findall(x)))\n",
    "    ncap_perc = ncap / nchar\n",
    "    nexcl = docs.apply(lambda x: len(re.compile(r\"!\").findall(x)))\n",
    "    nquest = docs.apply(lambda x: len(re.compile(r\"\\?\").findall(x)))\n",
    "    nsymb = docs.apply(lambda x: len(re.compile(r\"&|@|#|\\$|%|\\*|\\^\").findall(x)))\n",
    "    nsmile = docs.apply(lambda x: len(re.compile(r\"((?::|;|=)(?:-)?(?:\\)|D|P))\").findall(x)))\n",
    "    return pd.DataFrame(data = dict(\n",
    "        nwords = nwords, nchar = nchar, ncap = ncap, ncap_perc = ncap_perc,\n",
    "        nexcl = nexcl, nquest = nquest, nsymb = nsymb, nsmile = nsmile\n",
    "    ))\n",
    "\n",
    "pipeline = Pipeline(steps = [\n",
    "    ('features', FeatureUnion(transformer_list = [\n",
    "        ('w2v', W2VTransformer(workers = 1)),\n",
    "        ('tfidf', TfidfVectorizer(min_df = 3, max_df = 0.5)),\n",
    "        ('kbest', Pipeline(steps = [\n",
    "            ('cv', CountVectorizer(min_df = 3, max_df = 0.5)),\n",
    "            ('kbest', SelectKBest())\n",
    "        ])),\n",
    "        ('stats', FunctionTransformer(func = basic_stats, validate = False)),\n",
    "    ])),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('model', LogisticRegression(class_weight = \"balanced\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 5.61 s, total: 33.1 s\n",
      "Wall time: 5h 34min 35s\n",
      "Best params: {'features__kbest__kbest__k': 500, 'features__tfidf__max_features': 900, 'features__w2v__size': 800, 'model__C': 0.01, 'model__penalty': 'l1'}\n",
      "Best params score: 0.9551400733498527\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "param_grid = {\n",
    "    'features__w2v__size': (np.arange(1, 10) * 100).tolist(),\n",
    "    'features__tfidf__max_features': (np.arange(1, 10) * 100).tolist(),\n",
    "    'features__kbest__kbest__k': (np.arange(1, 8) * 250).tolist(),\n",
    "    'model__penalty': ['l1', 'l2'],\n",
    "    'model__C': [0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "try:\n",
    "    with open('model_param.yaml', 'r') as f:\n",
    "        param_optimal = yaml.load(f)\n",
    "except IOError:\n",
    "    param_optimal = {}\n",
    "\n",
    "    # create tuner\n",
    "    client = Client()\n",
    "    ss = ShuffleSplit(n_splits = 5, train_size = 0.25, random_state = 1)\n",
    "    tuner = RandomizedSearchCV(pipeline, param_grid, scheduler = client, scoring = 'roc_auc', \n",
    "                               cv = ss, random_state = 1, n_iter = 20)\n",
    "    \n",
    "    # use tuner to determine optimal params\n",
    "    %time tuner.fit(xdata_train, ydata_train)\n",
    "    print('Best params: %s' % (str(tuner.best_params_)))\n",
    "    print('Best params score: %s' % (str(tuner.best_score_)))\n",
    "\n",
    "    # save best params\n",
    "    param_optimal = tuner.best_params_\n",
    "    with open('model_param.yaml', 'w') as f:\n",
    "        yaml.dump(param_optimal, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 42s, sys: 3.87 s, total: 22min 46s\n",
      "Wall time: 22min 44s\n"
     ]
    }
   ],
   "source": [
    "# build model with optimal param\n",
    "pipeline.set_params(**param_optimal)\n",
    "%time model = pipeline.fit(xdata_train, ydata_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 50s, sys: 417 ms, total: 4min 50s\n",
      "Wall time: 4min 50s\n"
     ]
    }
   ],
   "source": [
    "# make predictions for our test set\n",
    "%time ydata_test_pred = model.predict_proba(xdata_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive threshold: 0.7525718174969368\n",
      "Confusion matrix:\n",
      "[[28046   766]\n",
      " [  766  2337]]\n",
      "AUC: 0.9614389452795857\n"
     ]
    }
   ],
   "source": [
    "# determine cutoff balancing precision/recall\n",
    "precision, recall, threshold = precision_recall_curve(ydata_test, ydata_test_pred)\n",
    "pos_threshold = np.min(threshold[precision[1:] > recall[:-1]])\n",
    "print('Positive threshold: %s' % str(pos_threshold))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(ydata_test, (ydata_test_pred >= pos_threshold).astype(int)))\n",
    "print('AUC: %s' % roc_auc_score(ydata_test, ydata_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
